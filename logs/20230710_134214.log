2023-07-10 13:42:14,812 - INFO - Ingesting data with config DataIngestionConfig(train_data_path=data\train, test_data_path=data\validation, load_images=<function load_image at 0x000002B5B8F85670>)
2023-07-10 13:42:16,593 - INFO - Transforming data with DataTransformationConfig(channel_features=['mean', 'std', 'median', 'mode', 'min', 'max', 'range', 'skewness', 'kurtosis', 'entropy', 'quantile_0.25', 'quantile_0.75', 'iqr'], histogram_features=['mean', 'std', 'median', 'mode', 'min', 'max', 'range', 'skewness', 'kurtosis', 'entropy', 'R'], coocurrence_matrix_features=['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation'], to_grayscale=<function to_grayscale at 0x000002B5B8F85790>, to_histogram=<function to_histogram at 0x000002B5B90849D0>)
2023-07-10 13:42:17,211 - INFO - Saving features in data\train_features.csv and data\test_features.csv
2023-07-10 13:42:17,226 - INFO - Training models with features with config ModelTrainerConfig(k_features_grid=[10, 20, 40, 'all'], feature_selection_score_function=<function mutual_info_classif at 0x000002B5B8F85280>, models=[Model(model_name=Logistic Regression, model_parameter_grid={'C': [0.1, 0.5, 1.0, 2.0, 5.0], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'max_iter': [200, 500, 1000], 'n_jobs': [-1], 'random_state': [42]}, model=LogisticRegression()), Model(model_name=Random Forest, model_parameter_grid={'n_estimators': [100, 200, 500, 1000], 'criterion': ['gini', 'entropy', 'log_loss'], 'max_depth': [None, 10, 20, 50, 100]}, model=RandomForestClassifier()), Model(model_name=KNN, model_parameter_grid={'n_neighbors': [3, 5, 7, 9, 11, 13, 15], 'weights': ['distance'], 'p': [1, 2], 'n_jobs': [-1]}, model=KNeighborsClassifier()), Model(model_name=SVM, model_parameter_grid={'C': [0.1, 0.5, 1.0, 2.0, 5.0], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'degree': [1, 2, 3, 4, 5], 'gamma': ['scale', 'auto'], 'max_iter': [200, 500, 1000], 'random_state': [42]}, model=SVC()), Model(model_name=Decision Tree, model_parameter_grid={'criterion': ['gini', 'entropy'], 'splitter': ['best'], 'max_depth': [None, 5, 10, 50, 100], 'random_state': [42]}, model=DecisionTreeClassifier())], target_column=label, score_criteria=f1_micro, cv=5)
